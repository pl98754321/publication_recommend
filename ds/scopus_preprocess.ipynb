{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import json\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_ref(ref:dict):\n",
    "    info = ref[\"ref-info\"]\n",
    "    if \"ref-title\" in info.keys():\n",
    "        if \"ref-publicationyear\" in info.keys():\n",
    "            year = info[\"ref-publicationyear\"][\"@first\"]\n",
    "        else:\n",
    "            year = \"unknow\"\n",
    "        return year,info[\"ref-title\"][\"ref-titletext\"]\n",
    "    return (None,None)\n",
    "def extract_pub(data:dict):\n",
    "    abstracts = data[\"head\"][\"abstracts\"]\n",
    "    title = data[\"head\"][\"citation-title\"]\n",
    "    sourcetitle = data[\"head\"][\"source\"][\"sourcetitle\"]\n",
    "    return title,sourcetitle,abstracts\n",
    "\n",
    "def extract_affiliation(data:dict):\n",
    "    def get_affiname(affiliation:list[dict]|dict,col:str):\n",
    "        if isinstance(affiliation,dict):\n",
    "            return [affiliation[col] if col in affiliation.keys() else None]\n",
    "        return [i[col] if col in i.keys() else None for i in affiliation] \n",
    "    \n",
    "    data = data[\"abstracts-retrieval-response\"][\"affiliation\"]\n",
    "    return get_affiname(data,\"affilname\"), get_affiname(data,\"affiliation-city\"),get_affiname(data,\"affiliation-country\"),\n",
    "\n",
    "def extract_json_file(file_name:str):\n",
    "    list_ref = []\n",
    "    \n",
    "    with open(file_name,encoding=\"utf8\") as f:\n",
    "        try:\n",
    "            data = json.load(f)\n",
    "        except:\n",
    "            print(file_name)\n",
    "            return None,None\n",
    "        affilname,affilcity,affilcountry = extract_affiliation(data)\n",
    "        data = data[\"abstracts-retrieval-response\"][\"item\"][\"bibrecord\"]\n",
    "        title,sourcetitle,abstracts = extract_pub(data)\n",
    "        new_pub = (title,sourcetitle,abstracts,affilname,affilcity,affilcountry)\n",
    "        if data[\"tail\"] is not None:\n",
    "            list_referece = data[\"tail\"][\"bibliography\"][\"reference\"]\n",
    "            if isinstance(list_referece,dict):\n",
    "                year_ref,title_ref = extract_ref(list_referece)\n",
    "                if year_ref != None and title_ref != None:\n",
    "                    list_ref.append((title,sourcetitle,year_ref,title_ref))\n",
    "            else:\n",
    "                for i in list_referece:\n",
    "                    year_ref,title_ref = extract_ref(i)\n",
    "                    if year_ref != None and title_ref != None:\n",
    "                        list_ref.append((title,sourcetitle,year_ref,title_ref))\n",
    "        return new_pub,list_ref"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/2792 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2792/2792 [00:08<00:00, 333.95it/s]\n",
      "100%|██████████| 3082/3082 [00:08<00:00, 373.17it/s]\n",
      "100%|██████████| 3393/3393 [00:08<00:00, 419.22it/s]\n",
      "100%|██████████| 3815/3815 [00:08<00:00, 430.63it/s]\n",
      "100%|██████████| 4244/4244 [00:09<00:00, 447.36it/s]\n",
      "100%|██████████| 2890/2890 [00:06<00:00, 472.15it/s]\n"
     ]
    }
   ],
   "source": [
    "list_ref = []\n",
    "list_pub = []\n",
    "\n",
    "for pub_year in os.listdir(r\"./Project\"):\n",
    "    for pub_name in tqdm(os.listdir(r\"./Project/\" + pub_year)):\n",
    "        file_name = r\"./Project/\" + pub_year + \"/\"+ pub_name\n",
    "        new_pub,list_single_ref = extract_json_file(file_name)\n",
    "        list_ref.extend(list_single_ref)\n",
    "        list_pub.append(new_pub)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ref_raw = pd.DataFrame(list_ref,columns=[\"title\",\"sourcetitle\",\"year\",\"title_ref\"])\n",
    "df_pub_raw = pd.DataFrame(list_pub,columns=[\"title\",\"sourcetitle\",\"abstracts\",\"affilname\",\"affilcity\",\"affilcountry\"])\n",
    "df_pub_raw[\"link\"] = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ref_raw.to_csv(\"ref_raW.csv\",index=False)\n",
    "df_pub_raw.to_csv(\"pub_raw.csv\",index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import ast\n",
    "# Create TF-IDF representation\n",
    "\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "def clean_text(text):\n",
    "    \n",
    "    # Remove text between parentheses\n",
    "    text = re.sub(r'\\([^)]*\\)', ' ', text)\n",
    "    \n",
    "    # Remove punctuation\n",
    "    text = re.sub(r'[^\\w\\s]', ' ', text)\n",
    "    \n",
    "\n",
    "    # Remove numbers\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "    \n",
    "    # Convert text to lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Tokenize text\n",
    "    tokens = word_tokenize(text)\n",
    "    \n",
    "    # Remove stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [word for word in tokens if word not in stop_words]\n",
    "    \n",
    "    # Lemmatization\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
    "    \n",
    "    # Join tokens back into a single string\n",
    "    cleaned_text = ' '.join(tokens)\n",
    "    \n",
    "    return cleaned_text\n",
    "\n",
    "def proprocess_pub_raw(df_pub_raw:pd.DataFrame):\n",
    "\n",
    "    df_pub_raw_explode = df_pub_raw.explode([\"affilname\",\"affilcity\",\"affilcountry\"])\n",
    "    affiliate_df = df_pub_raw_explode[[\"affilname\",\"affilcity\",\"affilcountry\"]].drop_duplicates().reset_index(drop=True).reset_index(names=\"id\")\n",
    "    df_pub_raw_explode = df_pub_raw_explode.merge(affiliate_df,how=\"left\",on=[\"affilname\",\"affilcity\",\"affilcountry\"])\n",
    "    df_pub_merge_aff = df_pub_raw_explode.groupby(\"title\")[\"id\"].apply(list).to_frame().reset_index()\n",
    "    df_pub_raw = df_pub_raw.merge(df_pub_merge_aff,on=\"title\")[[\"title\",\"abstracts\",\"id\",\"link\"]].rename(columns={\"id\":\"affiliation_id\"})\n",
    "    df_pub_raw = df_pub_raw.reset_index(names=\"id\")\n",
    "    \n",
    "    return df_pub_raw,affiliate_df\n",
    "\n",
    "def preprocess_df_ref(df_ref:pd.DataFrame,df_pub:pd.DataFrame):\n",
    "    df_ref[\"title_ref\"] = df_ref[\"title_ref\"].str.lower()\n",
    "    df_count = df_ref.groupby(\"title_ref\").count()\n",
    "    df_ref_new = df_ref[df_ref[\"title_ref\"].isin(df_count[df_count[\"title\"] > 1].index)]\n",
    "    df_ref_new = df_ref_new.drop(columns=[\"sourcetitle\"])\n",
    "    df_merge = df_ref_new.merge(df_ref_new[[\"title_ref\",\"title\"]],on=[\"title_ref\"],how=\"left\",suffixes=('','_2'))\n",
    "    df_merge = df_merge[df_merge[\"title\"] != df_merge[\"title_2\"]]\n",
    "    group_fre = df_merge.groupby([\"title\",\"title_2\"]).count().sort_values(\"title_ref\",ascending=False).reset_index()\n",
    "    group_fre.columns = [\"title\",\"title_2\",\"count\",\"count_2\"]\n",
    "    group_fre = group_fre.drop(columns=[\"count_2\"])\n",
    "    df_dropdup = df_merge.drop_duplicates(subset=[\"title\",\"title_2\"])\n",
    "    df_dropdup = df_dropdup.merge(group_fre,on=[\"title\",\"title_2\"],how=\"left\")\n",
    "\n",
    "    df_ref_merge_id = df_dropdup.merge(df_pub[[\"title\",\"id\"]],on=\"title\",how=\"left\")\n",
    "    df_ref_merge_id = df_ref_merge_id[df_ref_merge_id.id.notna()]\n",
    "    df_ref_merge_id = df_ref_merge_id.merge(df_pub[[\"title\",\"id\"]],left_on=\"title_2\",right_on=\"title\",how=\"inner\",suffixes=('','_3'))\n",
    "    df_ref_merge_id = df_ref_merge_id[[\"id\",\"id_3\",\"count\",\"year\"]]\n",
    "    df_ref_merge_id = df_ref_merge_id.astype({\"id\":\"int\",\"id_3\":\"int\",\"count\":\"int\"})\n",
    "    df_ref_merge_id.columns = [\"source\",\"target\",\"weight\",\"year\"]\n",
    "    return df_ref_merge_id\n",
    "\n",
    "\n",
    "def create_similarity_matrix(list_abstracts:list[str]):\n",
    "    # Create TF-IDF representation\n",
    "    tfidf_vectorizer = TfidfVectorizer()\n",
    "    tfidf_matrix = tfidf_vectorizer.fit_transform(list_abstracts)\n",
    "    similarity_matrix = cosine_similarity(tfidf_matrix, tfidf_matrix)\n",
    "    return similarity_matrix\n",
    "def turn_id_to_idx(pub_id:int):\n",
    "    return pub_id\n",
    "\n",
    "def turn_idx_to_id(pub_idx:int):\n",
    "    return pub_idx\n",
    "\n",
    "def recommend_publications(pub_id:int, similarity_matrix, page_size:int=10, page:int=1):\n",
    "    # Get similarity scores\n",
    "    \n",
    "    similarity_scores = similarity_matrix[pub_id]\n",
    "    \n",
    "    # Get indices of publications similar to the target publication\n",
    "    similar_pub_indices = similarity_scores.argsort()[::-1]\n",
    "    similar_pub_indices = similar_pub_indices[similar_pub_indices != pub_id]\n",
    "    similar_pub_indices = similar_pub_indices[(page-1)*page_size:page*page_size]\n",
    "    similar_pub_indices = [turn_idx_to_id(idx) for idx in similar_pub_indices]\n",
    "    return similar_pub_indices\n",
    "\n",
    "def add_rec_to_pub(df_pub:pd.DataFrame):\n",
    "    df_pub[\"abstracts_preprocess\"] = df_pub[\"abstracts\"].apply(clean_text)\n",
    "    similiar_matrix = create_similarity_matrix(df_pub[\"abstracts_preprocess\"].to_list())\n",
    "    df_pub = df_pub.drop(columns=\"id\").reset_index(drop=True).reset_index(names=\"id\")\n",
    "    df_pub[\"rec\"] = df_pub[\"id\"].apply(lambda x : recommend_publications(x,similiar_matrix))\n",
    "    df_pub = df_pub.drop(columns=\"abstracts_preprocess\")\n",
    "    return df_pub\n",
    "\n",
    "def load_data():\n",
    "    import pandas as pd\n",
    "    lat_lon_ciry = pd.DataFrame(columns=[\"affilcity\",\"lat\",\"lon\"])\n",
    "    df_ref = pd.read_csv(\"ref_raW.csv\")\n",
    "    df_pub = pd.read_csv(\"pub_raw.csv\")\n",
    "    \n",
    "    df_pub = df_pub[(df_pub[\"abstracts\"].notna()) & (df_pub[\"title\"].notna())]\n",
    "    df_pub[\"len\"] = df_pub[\"title\"].str.len()\n",
    "    for col in [\"affilname\",\"affilcity\",\"affilcountry\"]:\n",
    "        df_pub[col] = df_pub[col].apply(ast.literal_eval)\n",
    "    return df_ref,df_pub,lat_lon_ciry\n",
    "\n",
    "def main():\n",
    "    print(\"Start\")\n",
    "    df_ref,df_pub,lat_lon_ciry = load_data()\n",
    "    print(\"Data loaded\")\n",
    "    df_pub,affiliate_df = proprocess_pub_raw(df_pub)\n",
    "    affiliate_df = affiliate_df.merge(lat_lon_ciry,on=\"affilcity\",how=\"left\")\n",
    "    print(\"Data preprocessed\")\n",
    "    df_pub_preprocess = add_rec_to_pub(df_pub)\n",
    "    print(\"Recommendation added\")\n",
    "    df_ref_merge_id = preprocess_df_ref(df_ref,df_pub)\n",
    "    print(\"Save data\")\n",
    "    affiliate_df.to_csv(\"./data_process/affiliation.csv\",index=False)\n",
    "    df_pub_preprocess.to_csv(\"./data_process/pub_preprocessed.csv\",index=False)\n",
    "    df_ref_merge_id.to_csv(\"./data_process/ref_preprocessed.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start\n",
      "Data loaded\n",
      "Data preprocessed\n",
      "Recommendation added\n",
      "Save data\n"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TATA preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "dftp = pd.read_csv(r\"C:\\codepluem\\CU\\datasci_proj\\webscraping\\research_csv\\06D05M2024Y\\paper_info.csv\")\n",
    "dftp.drop(columns=[\"Unnamed: 0\"],inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'@_fa': 'true',\n",
       "  'affilname': 'European Investment Bank',\n",
       "  'affiliation-city': 'Luxembourg',\n",
       "  'affiliation-country': 'Luxembourg'}]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "json.loads(str(dftp[\"affiliation\"].iloc[0].replace(\"\\'\",\"\\\"\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
