{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import json\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_ref(ref:dict):\n",
    "    info = ref[\"ref-info\"]\n",
    "    if \"ref-title\" in info.keys():\n",
    "        if \"ref-publicationyear\" in info.keys():\n",
    "            year = info[\"ref-publicationyear\"][\"@first\"]\n",
    "        else:\n",
    "            year = \"unknow\"\n",
    "        return year,info[\"ref-title\"][\"ref-titletext\"]\n",
    "    return (None,None)\n",
    "def extract_pub(data:dict):\n",
    "    abstracts = data[\"head\"][\"abstracts\"]\n",
    "    title = data[\"head\"][\"citation-title\"]\n",
    "    sourcetitle = data[\"head\"][\"source\"][\"sourcetitle\"]\n",
    "    return title,sourcetitle,abstracts\n",
    "\n",
    "def extract_affiliation(data:dict):\n",
    "    def get_affiname(affiliation:list[dict]|dict,col:str):\n",
    "        if isinstance(affiliation,dict):\n",
    "            return [affiliation[col] if col in affiliation.keys() else None]\n",
    "        return [i[col] if col in i.keys() else None for i in affiliation] \n",
    "    \n",
    "    data = data[\"abstracts-retrieval-response\"][\"affiliation\"]\n",
    "    return get_affiname(data,\"affilname\"), get_affiname(data,\"affiliation-city\"),get_affiname(data,\"affiliation-country\"),\n",
    "\n",
    "def extract_json_file(file_name:str):\n",
    "    list_ref = []\n",
    "    \n",
    "    with open(file_name,encoding=\"utf8\") as f:\n",
    "        try:\n",
    "            data = json.load(f)\n",
    "        except:\n",
    "            print(file_name)\n",
    "            return None,None\n",
    "        affilname,affilcity,affilcountry = extract_affiliation(data)\n",
    "        data = data[\"abstracts-retrieval-response\"][\"item\"][\"bibrecord\"]\n",
    "        title,sourcetitle,abstracts = extract_pub(data)\n",
    "        new_pub = (title,sourcetitle,abstracts,affilname,affilcity,affilcountry)\n",
    "        if data[\"tail\"] is not None:\n",
    "            list_referece = data[\"tail\"][\"bibliography\"][\"reference\"]\n",
    "            if isinstance(list_referece,dict):\n",
    "                year_ref,title_ref = extract_ref(list_referece)\n",
    "                if year_ref != None and title_ref != None:\n",
    "                    list_ref.append((title,sourcetitle,year_ref,title_ref))\n",
    "            else:\n",
    "                for i in list_referece:\n",
    "                    year_ref,title_ref = extract_ref(i)\n",
    "                    if year_ref != None and title_ref != None:\n",
    "                        list_ref.append((title,sourcetitle,year_ref,title_ref))\n",
    "        return new_pub,list_ref"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/2792 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2792/2792 [00:08<00:00, 333.95it/s]\n",
      "100%|██████████| 3082/3082 [00:08<00:00, 373.17it/s]\n",
      "100%|██████████| 3393/3393 [00:08<00:00, 419.22it/s]\n",
      "100%|██████████| 3815/3815 [00:08<00:00, 430.63it/s]\n",
      "100%|██████████| 4244/4244 [00:09<00:00, 447.36it/s]\n",
      "100%|██████████| 2890/2890 [00:06<00:00, 472.15it/s]\n"
     ]
    }
   ],
   "source": [
    "list_ref = []\n",
    "list_pub = []\n",
    "\n",
    "for pub_year in os.listdir(r\"./Project\"):\n",
    "    for pub_name in tqdm(os.listdir(r\"./Project/\" + pub_year)):\n",
    "        file_name = r\"./Project/\" + pub_year + \"/\"+ pub_name\n",
    "        new_pub,list_single_ref = extract_json_file(file_name)\n",
    "        list_ref.extend(list_single_ref)\n",
    "        list_pub.append(new_pub)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ref_raw = pd.DataFrame(list_ref,columns=[\"title\",\"sourcetitle\",\"year\",\"title_ref\"])\n",
    "df_pub_raw = pd.DataFrame(list_pub,columns=[\"title\",\"sourcetitle\",\"abstracts\",\"affilname\",\"affilcity\",\"affilcountry\"])\n",
    "df_pub_raw[\"link\"] = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ref_raw.to_csv(\"ref_raW.csv\",index=False)\n",
    "df_pub_raw.to_csv(\"pub_raw.csv\",index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import ast\n",
    "import pandas as pd\n",
    "# Create TF-IDF representation\n",
    "\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "from pathlib import Path\n",
    "import ast\n",
    "\n",
    "def clean_text(text):\n",
    "    \n",
    "    # Remove text between parentheses\n",
    "    text = re.sub(r'\\([^)]*\\)', ' ', text)\n",
    "    \n",
    "    # Remove punctuation\n",
    "    text = re.sub(r'[^\\w\\s]', ' ', text)\n",
    "    \n",
    "\n",
    "    # Remove numbers\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "    \n",
    "    # Convert text to lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Tokenize text\n",
    "    tokens = word_tokenize(text)\n",
    "    \n",
    "    # Remove stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [word for word in tokens if word not in stop_words]\n",
    "    \n",
    "    # Lemmatization\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
    "    \n",
    "    # Join tokens back into a single string\n",
    "    cleaned_text = ' '.join(tokens)\n",
    "    \n",
    "    return cleaned_text\n",
    "\n",
    "def proprocess_pub_raw(df_pub_raw:pd.DataFrame):\n",
    "\n",
    "    df_pub_raw_explode = df_pub_raw.explode([\"affilname\",\"affilcity\",\"affilcountry\"])\n",
    "    affiliate_df = df_pub_raw_explode[[\"affilname\",\"affilcity\",\"affilcountry\"]].drop_duplicates().reset_index(drop=True).reset_index(names=\"id\")\n",
    "    df_pub_raw_explode = df_pub_raw_explode.merge(affiliate_df,how=\"left\",on=[\"affilname\",\"affilcity\",\"affilcountry\"])\n",
    "    df_pub_merge_aff = df_pub_raw_explode.groupby(\"title\")[\"id\"].apply(list).to_frame().reset_index()\n",
    "    df_pub_raw = df_pub_raw.merge(df_pub_merge_aff,on=\"title\")[[\"title\",\"abstracts\",\"id\",\"link\"]].rename(columns={\"id\":\"affiliation_id\"})\n",
    "    df_pub_raw = df_pub_raw.reset_index(names=\"id\")\n",
    "    \n",
    "    return df_pub_raw,affiliate_df\n",
    "\n",
    "def preprocess_df_ref(df_ref:pd.DataFrame,df_pub:pd.DataFrame):\n",
    "    df_ref[\"title_ref\"] = df_ref[\"title_ref\"].str.lower()\n",
    "    df_count = df_ref.groupby(\"title_ref\").count()\n",
    "    df_ref_new = df_ref[df_ref[\"title_ref\"].isin(df_count[df_count[\"title\"] > 1].index)]\n",
    "    df_merge = df_ref_new.merge(df_ref_new[[\"title_ref\",\"title\"]],on=[\"title_ref\"],how=\"left\",suffixes=('','_2'))\n",
    "    df_merge = df_merge[df_merge[\"title\"] != df_merge[\"title_2\"]]\n",
    "    group_fre = df_merge.groupby([\"title\",\"title_2\"]).count().sort_values(\"title_ref\",ascending=False).reset_index()\n",
    "    group_fre.columns = [\"title\",\"title_2\",\"count\",\"count_2\"]\n",
    "    group_fre = group_fre.drop(columns=[\"count_2\"])\n",
    "    df_dropdup = df_merge.drop_duplicates(subset=[\"title\",\"title_2\"])\n",
    "    df_dropdup = df_dropdup.merge(group_fre,on=[\"title\",\"title_2\"],how=\"left\")\n",
    "\n",
    "    df_ref_merge_id = df_dropdup.merge(df_pub[[\"title\",\"id\"]],on=\"title\",how=\"left\")\n",
    "    df_ref_merge_id = df_ref_merge_id[df_ref_merge_id.id.notna()]\n",
    "    df_ref_merge_id = df_ref_merge_id.merge(df_pub[[\"title\",\"id\"]],left_on=\"title_2\",right_on=\"title\",how=\"inner\",suffixes=('','_3'))\n",
    "    df_ref_merge_id = df_ref_merge_id[[\"id\",\"id_3\",\"count\",\"year\"]]\n",
    "    df_ref_merge_id = df_ref_merge_id.astype({\"id\":\"int\",\"id_3\":\"int\",\"count\":\"int\"})\n",
    "    df_ref_merge_id.columns = [\"source\",\"target\",\"weight\",\"year\"]\n",
    "    return df_ref_merge_id\n",
    "\n",
    "\n",
    "def create_similarity_matrix(list_abstracts:list[str]):\n",
    "    # Create TF-IDF representation\n",
    "    tfidf_vectorizer = TfidfVectorizer()\n",
    "    tfidf_matrix = tfidf_vectorizer.fit_transform(list_abstracts)\n",
    "    similarity_matrix = cosine_similarity(tfidf_matrix, tfidf_matrix)\n",
    "    return similarity_matrix\n",
    "def turn_id_to_idx(pub_id:int):\n",
    "    return pub_id\n",
    "\n",
    "def turn_idx_to_id(pub_idx:int):\n",
    "    return pub_idx\n",
    "\n",
    "def recommend_publications(pub_id:int, similarity_matrix, page_size:int=10, page:int=1):\n",
    "    # Get similarity scores\n",
    "    \n",
    "    similarity_scores = similarity_matrix[pub_id]\n",
    "    \n",
    "    # Get indices of publications similar to the target publication\n",
    "    similar_pub_indices = similarity_scores.argsort()[::-1]\n",
    "    similar_pub_indices = similar_pub_indices[similar_pub_indices != pub_id]\n",
    "    similar_pub_indices = similar_pub_indices[(page-1)*page_size:page*page_size]\n",
    "    similar_pub_indices = [turn_idx_to_id(idx) for idx in similar_pub_indices]\n",
    "    return similar_pub_indices\n",
    "\n",
    "def add_rec_to_pub(df_pub:pd.DataFrame):\n",
    "    df_pub[\"abstracts_preprocess\"] = df_pub[\"abstracts\"].apply(clean_text)\n",
    "    similiar_matrix = create_similarity_matrix(df_pub[\"abstracts_preprocess\"].to_list())\n",
    "    df_pub = df_pub.drop(columns=\"id\").reset_index(drop=True).reset_index(names=\"id\")\n",
    "    df_pub[\"rec\"] = df_pub[\"id\"].apply(lambda x : recommend_publications(x,similiar_matrix))\n",
    "    df_pub = df_pub.drop(columns=\"abstracts_preprocess\")\n",
    "    return df_pub\n",
    "\n",
    "def load_data():\n",
    "    import pandas as pd\n",
    "    # lat_lon_ciry = pd.DataFrame(columns=[\"affilcity\",\"lat\",\"lon\"])\n",
    "    lat_lon_ciry = pd.read_csv(\"./data_raw/worldcities.csv\")\n",
    "    df_ref = pd.read_csv(\"./data_raw/ref_raw.csv\")\n",
    "    df_pub = pd.read_csv(\"./data_raw/pub_raw.csv\")\n",
    "    \n",
    "    lat_lon_ciry = lat_lon_ciry[[\"city_ascii\",\"lat\",\"lng\"]].rename(columns={\"city_ascii\":\"affilcity\",\"lat\":\"lat\",\"lng\":\"lon\"})\n",
    "    lat_lon_ciry = lat_lon_ciry.drop_duplicates(\"affilcity\")\n",
    "    lat_lon_ciry[\"lat\"] = lat_lon_ciry[\"lat\"].astype(float)\n",
    "    lat_lon_ciry[\"lon\"] = lat_lon_ciry[\"lon\"].astype(float)\n",
    "    \n",
    "    if \"sourcetitle\" in df_ref.columns:\n",
    "        df_ref = df_ref.drop(columns=\"sourcetitle\")\n",
    "    if \"sourcetitle\" in df_pub.columns:\n",
    "        df_pub = df_pub.drop(columns=\"sourcetitle\")\n",
    "    \n",
    "    \n",
    "    df_pub = df_pub[(df_pub[\"abstracts\"].notna()) & (df_pub[\"title\"].notna())]\n",
    "    df_pub[\"len\"] = df_pub[\"title\"].str.len()\n",
    "    for col in [\"affilname\",\"affilcity\",\"affilcountry\"]:\n",
    "        df_pub[col] = df_pub[col].apply(ast.literal_eval)\n",
    "    return df_ref,df_pub,lat_lon_ciry\n",
    "\n",
    "def load_new_pub(folder:str):\n",
    "    dftp = pd.read_csv(Path(folder)/ \"paper_info.csv\")\n",
    "    dfref = pd.read_csv(Path(folder)/ \"ref_info.csv\")\n",
    "    \n",
    "    # preprocess dftp\n",
    "    dfref = dfref.merge(dftp[[\"id\",\"title\"]])\n",
    "    dfref = dfref[[\"title\",\"ref_publicationyear\",\"ref_title\"]].rename(columns={\"ref_publicationyear\":\"year\",\"ref_title\":\"title_ref\"})\n",
    "    dfref = dfref[dfref[\"title_ref\"].notna()]\n",
    "    \n",
    "    # preprocess dftp\n",
    "    dftp.drop(columns=[\"Unnamed: 0\"],inplace=True)\n",
    "    dftp = dftp[[\"title\",\"abstract\",\"affiliation\",\"link\"]].rename(columns={\"abstract\":\"abstracts\"})\n",
    "    dftp[\"affiliation\"] = dftp[\"affiliation\"].apply(ast.literal_eval)\n",
    "    dftp[\"affilname\"] = dftp[\"affiliation\"].apply(lambda x : [i[0] for i in x])\n",
    "    dftp[\"affilcity\"] = dftp[\"affiliation\"].apply(lambda x : [i[1] for i in x])\n",
    "    dftp[\"affilcountry\"] = dftp[\"affiliation\"].apply(lambda x : [i[2] for i in x])\n",
    "    dftp = dftp.drop(columns=[\"affiliation\"])\n",
    "    \n",
    "    return dftp,dfref\n",
    "\n",
    "def merge_save_new_pub(df_pub:pd.DataFrame,df_ref:pd.DataFrame,df_pub_new:pd.DataFrame,df_ref_new:pd.DataFrame):\n",
    "    df_pub = pd.concat([df_pub,df_pub_new])\n",
    "    df_ref = pd.concat([df_ref,df_ref_new])\n",
    "    df_pub = df_pub.drop_duplicates(subset=[\"title\"])\n",
    "    df_ref = df_ref.drop_duplicates(subset=[\"title\",\"title_ref\"])\n",
    "    \n",
    "    df_pub = df_pub[df_pub[\"title\"].notna() & df_pub[\"abstracts\"].notna()]\n",
    "    df_pub.to_csv(\"./data_raw/pub_raw.csv\",index=False)\n",
    "    df_ref.to_csv(\"./data_raw/ref_raw.csv\",index=False)\n",
    "    return df_pub,df_ref\n",
    "\n",
    "\n",
    "def main(folder:str):\n",
    "    print(\"Start\")\n",
    "    df_ref,df_pub,lat_lon_ciry = load_data()\n",
    "    df_pub_new ,df_ref_new =load_new_pub(folder)\n",
    "    df_pub,df_ref = merge_save_new_pub(df_pub,df_ref,df_pub_new,df_ref_new)\n",
    "    print(\"Data loaded\")\n",
    "    df_pub,affiliate_df = proprocess_pub_raw(df_pub)\n",
    "    affiliate_df = affiliate_df.merge(lat_lon_ciry,on=\"affilcity\",how=\"left\")\n",
    "    print(\"Data preprocessed\")\n",
    "    df_pub_preprocess = add_rec_to_pub(df_pub)\n",
    "    print(\"Recommendation added\")\n",
    "    df_ref_merge_id = preprocess_df_ref(df_ref,df_pub)\n",
    "    print(\"Save data\")\n",
    "    affiliate_df.to_csv(\"./data_process/affiliation.csv\",index=False)\n",
    "    df_pub_preprocess.to_csv(\"./data_process/pub_preprocessed.csv\",index=False)\n",
    "    df_ref_merge_id.to_csv(\"./data_process/ref_preprocessed.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start\n",
      "Data loaded\n",
      "Data preprocessed\n",
      "Recommendation added\n",
      "Save data\n"
     ]
    }
   ],
   "source": [
    "main(r\"C:\\codepluem\\CU\\datasci_proj\\webscraping\\research_csv\\09D05M2024Y\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TATA preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "ww = pd.read_csv(r\"C:\\codepluem\\CU\\datasci_proj\\ds\\data_raw\\worldcities.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>city</th>\n",
       "      <th>city_ascii</th>\n",
       "      <th>lat</th>\n",
       "      <th>lng</th>\n",
       "      <th>country</th>\n",
       "      <th>iso2</th>\n",
       "      <th>iso3</th>\n",
       "      <th>admin_name</th>\n",
       "      <th>capital</th>\n",
       "      <th>population</th>\n",
       "      <th>id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Tokyo</td>\n",
       "      <td>Tokyo</td>\n",
       "      <td>35.6897</td>\n",
       "      <td>139.6922</td>\n",
       "      <td>Japan</td>\n",
       "      <td>JP</td>\n",
       "      <td>JPN</td>\n",
       "      <td>Tōkyō</td>\n",
       "      <td>primary</td>\n",
       "      <td>37732000.0</td>\n",
       "      <td>1392685764</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Jakarta</td>\n",
       "      <td>Jakarta</td>\n",
       "      <td>-6.1750</td>\n",
       "      <td>106.8275</td>\n",
       "      <td>Indonesia</td>\n",
       "      <td>ID</td>\n",
       "      <td>IDN</td>\n",
       "      <td>Jakarta</td>\n",
       "      <td>primary</td>\n",
       "      <td>33756000.0</td>\n",
       "      <td>1360771077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Delhi</td>\n",
       "      <td>Delhi</td>\n",
       "      <td>28.6100</td>\n",
       "      <td>77.2300</td>\n",
       "      <td>India</td>\n",
       "      <td>IN</td>\n",
       "      <td>IND</td>\n",
       "      <td>Delhi</td>\n",
       "      <td>admin</td>\n",
       "      <td>32226000.0</td>\n",
       "      <td>1356872604</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Guangzhou</td>\n",
       "      <td>Guangzhou</td>\n",
       "      <td>23.1300</td>\n",
       "      <td>113.2600</td>\n",
       "      <td>China</td>\n",
       "      <td>CN</td>\n",
       "      <td>CHN</td>\n",
       "      <td>Guangdong</td>\n",
       "      <td>admin</td>\n",
       "      <td>26940000.0</td>\n",
       "      <td>1156237133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Mumbai</td>\n",
       "      <td>Mumbai</td>\n",
       "      <td>19.0761</td>\n",
       "      <td>72.8775</td>\n",
       "      <td>India</td>\n",
       "      <td>IN</td>\n",
       "      <td>IND</td>\n",
       "      <td>Mahārāshtra</td>\n",
       "      <td>admin</td>\n",
       "      <td>24973000.0</td>\n",
       "      <td>1356226629</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47863</th>\n",
       "      <td>Munha-dong</td>\n",
       "      <td>Munha-dong</td>\n",
       "      <td>39.3813</td>\n",
       "      <td>127.2517</td>\n",
       "      <td>Korea, North</td>\n",
       "      <td>KP</td>\n",
       "      <td>PRK</td>\n",
       "      <td>Kangwŏn</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1408979215</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47864</th>\n",
       "      <td>Sil-li</td>\n",
       "      <td>Sil-li</td>\n",
       "      <td>39.4880</td>\n",
       "      <td>125.4640</td>\n",
       "      <td>Korea, North</td>\n",
       "      <td>KP</td>\n",
       "      <td>PRK</td>\n",
       "      <td>P’yŏngnam</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1408767958</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47865</th>\n",
       "      <td>Muan</td>\n",
       "      <td>Muan</td>\n",
       "      <td>34.9897</td>\n",
       "      <td>126.4714</td>\n",
       "      <td>Korea, South</td>\n",
       "      <td>KR</td>\n",
       "      <td>KOR</td>\n",
       "      <td>Jeonnam</td>\n",
       "      <td>admin</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1410001061</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47866</th>\n",
       "      <td>Hongseong</td>\n",
       "      <td>Hongseong</td>\n",
       "      <td>36.6009</td>\n",
       "      <td>126.6650</td>\n",
       "      <td>Korea, South</td>\n",
       "      <td>KR</td>\n",
       "      <td>KOR</td>\n",
       "      <td>Chungnam</td>\n",
       "      <td>admin</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1410822139</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47867</th>\n",
       "      <td>Charlotte Amalie</td>\n",
       "      <td>Charlotte Amalie</td>\n",
       "      <td>18.3420</td>\n",
       "      <td>-64.9331</td>\n",
       "      <td>U.S. Virgin Islands</td>\n",
       "      <td>VI</td>\n",
       "      <td>VIR</td>\n",
       "      <td>Virgin Islands</td>\n",
       "      <td>primary</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1850037473</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>44156 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                   city        city_ascii      lat       lng  \\\n",
       "0                 Tokyo             Tokyo  35.6897  139.6922   \n",
       "1               Jakarta           Jakarta  -6.1750  106.8275   \n",
       "2                 Delhi             Delhi  28.6100   77.2300   \n",
       "3             Guangzhou         Guangzhou  23.1300  113.2600   \n",
       "4                Mumbai            Mumbai  19.0761   72.8775   \n",
       "...                 ...               ...      ...       ...   \n",
       "47863        Munha-dong        Munha-dong  39.3813  127.2517   \n",
       "47864            Sil-li            Sil-li  39.4880  125.4640   \n",
       "47865              Muan              Muan  34.9897  126.4714   \n",
       "47866         Hongseong         Hongseong  36.6009  126.6650   \n",
       "47867  Charlotte Amalie  Charlotte Amalie  18.3420  -64.9331   \n",
       "\n",
       "                   country iso2 iso3      admin_name  capital  population  \\\n",
       "0                    Japan   JP  JPN           Tōkyō  primary  37732000.0   \n",
       "1                Indonesia   ID  IDN         Jakarta  primary  33756000.0   \n",
       "2                    India   IN  IND           Delhi    admin  32226000.0   \n",
       "3                    China   CN  CHN       Guangdong    admin  26940000.0   \n",
       "4                    India   IN  IND     Mahārāshtra    admin  24973000.0   \n",
       "...                    ...  ...  ...             ...      ...         ...   \n",
       "47863         Korea, North   KP  PRK         Kangwŏn      NaN         NaN   \n",
       "47864         Korea, North   KP  PRK       P’yŏngnam      NaN         NaN   \n",
       "47865         Korea, South   KR  KOR         Jeonnam    admin         NaN   \n",
       "47866         Korea, South   KR  KOR        Chungnam    admin         NaN   \n",
       "47867  U.S. Virgin Islands   VI  VIR  Virgin Islands  primary         NaN   \n",
       "\n",
       "               id  \n",
       "0      1392685764  \n",
       "1      1360771077  \n",
       "2      1356872604  \n",
       "3      1156237133  \n",
       "4      1356226629  \n",
       "...           ...  \n",
       "47863  1408979215  \n",
       "47864  1408767958  \n",
       "47865  1410001061  \n",
       "47866  1410822139  \n",
       "47867  1850037473  \n",
       "\n",
       "[44156 rows x 11 columns]"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ww.drop_duplicates(\"city_ascii\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "dftp = pd.read_csv(r\"C:\\codepluem\\CU\\datasci_proj\\webscraping\\research_csv\\09D05M2024Y\\paper_info.csv\")\n",
    "dftp.drop(columns=[\"Unnamed: 0\"],inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ref_raw = pd.DataFrame(list_ref,columns=[\"title\",\"sourcetitle\",\"year\",\"title_ref\"])\n",
    "df_pub_raw = pd.DataFrame(list_pub,columns=[\"title\",\"sourcetitle\",\"abstracts\",\"affilname\",\"affilcity\",\"affilcountry\"])\n",
    "df_pub_raw[\"link\"] = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import ast\n",
    "def load_new_pub(folder:str):\n",
    "    dftp = pd.read_csv(Path(folder)/ \"paper_info.csv\")\n",
    "    dfref = pd.read_csv(Path(folder)/ \"ref_info.csv\")\n",
    "    \n",
    "    # preprocess dftp\n",
    "    dfref = dfref.merge(dftp[[\"id\",\"title\"]])\n",
    "    dfref = dfref[[\"title\",\"ref_publicationyear\",\"ref_title\"]].rename(columns={\"ref_publicationyear\":\"year\",\"ref_title\":\"title_ref\"})\n",
    "    dfref = dfref[dfref[\"title_ref\"].notna()]\n",
    "    \n",
    "    # preprocess dftp\n",
    "    dftp.drop(columns=[\"Unnamed: 0\"],inplace=True)\n",
    "    dftp = dftp[[\"title\",\"abstract\",\"affiliation\",\"link\"]]\n",
    "    dftp[\"affiliation\"] = dftp[\"affiliation\"].apply(ast.literal_eval)\n",
    "    dftp[\"affilname\"] = dftp[\"affiliation\"].apply(lambda x : [i[0] for i in x])\n",
    "    dftp[\"affilcity\"] = dftp[\"affiliation\"].apply(lambda x : [i[1] for i in x])\n",
    "    dftp[\"affilcountry\"] = dftp[\"affiliation\"].apply(lambda x : [i[2] for i in x])\n",
    "    dftp = dftp.drop(columns=[\"affiliation\"])\n",
    "    \n",
    "    return dftp,dfref"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfref = pd.read_csv(r\"C:\\codepluem\\CU\\datasci_proj\\webscraping\\research_csv\\09D05M2024Y\\ref_info.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "dftp = pd.read_csv(r\"C:\\codepluem\\CU\\datasci_proj\\webscraping\\research_csv\\09D05M2024Y\\paper_info.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pub_new,df_ref_new=load_new_pub(r\"C:\\codepluem\\CU\\datasci_proj\\webscraping\\research_csv\\09D05M2024Y\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pub_new"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
